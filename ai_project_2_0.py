# -*- coding: utf-8 -*-
"""AI_Project_2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15TFGmg6TFrzmwTNIeh1gXiT2T8J_ALFG

# Importing Data and Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb
!pip install spacy
import spacy
import sqlite3
import seaborn as sns
import re,string,unicodedata
import pandas as pd
!pip install pycountry
import pycountry
import numpy as np
import nltk
import matplotlib.pyplot as plt
import base64
import matplotlib.gridspec as gridspec
from collections import Counter
from xgboost import XGBClassifier
from sklearn.svm import SVC
from wordcloud import WordCloud,STOPWORDS
from tqdm import tqdm
from nltk.corpus import stopwords
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.metrics import log_loss, f1_score
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from sklearn import metrics
from pprint import pprint
from sklearn.metrics import plot_confusion_matrix
from prettytable import PrettyTable
from numpy import math
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer,WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk import pos_tag
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from bs4 import BeautifulSoup
# %matplotlib inline
pd.set_option('display.max_columns', None)

from google.colab import drive
drive.mount('/content/gdrive')
import pandas as pd
df=pd.read_csv('gdrive/My Drive/fake_job_postings.csv')

sns.heatmap(df.isnull(),yticklabels=False)

df.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)

"""# Cleaning the Data"""

df.shape

df.nunique()

df.isna().sum() / len(df)

df2 = df.copy()

df2.drop(['salary_range', 'job_id', 'department', 'benefits'], axis = 1, inplace = True)

df2.head()

df2 = df2.sort_values('title').reset_index(drop = True)

df2.isna().sum()

df2['employment_type'] = df2['employment_type'].bfill(axis=0)
df2['required_experience'] = df2['required_experience'].bfill(axis=0)
df2['required_education'] = df2['required_education'].bfill(axis = 0)
df2['industry'] = df2['industry'].bfill(axis=0)
df2['function'] = df2['function'].bfill(axis=0)

df3 = df2.copy()

df3 = df3[df3['description'].notna()]

df3.isna().sum()

df3 = df3.dropna(axis = 0, how = 'any')

df3.shape

df3 = df3.drop_duplicates(keep = 'first')

df4 = df3.copy()

df4.head()

df4.shape

df4['description'] = df4['description'] + ' ' + df4['requirements'] + ' ' + df4['company_profile']
df4.drop(['company_profile', 'requirements'], axis = 1, inplace = True)

df4.head(3)

df4['country_code'] = df4['location'].str.split(',', expand=True)[0]
df4['city'] = df4['location'].str.split(',', expand = True)[2]

df4.head()

df4.loc[df4['city'] == ' ', 'city'] = np.nan

df4.isnull().sum()

df4.dropna(inplace = True)

list_alpha_2 = [i.alpha_2 for i in list(pycountry.countries)]
def country(df):
    if df['country_code'] in list_alpha_2:
        return pycountry.countries.get(alpha_2 = df['country_code']).name
df4['country_name'] = df4.apply(country, axis = 1)

df4.drop(['location', 'country_code'], axis = 1, inplace = True)

df4.head()

df4.shape

plt.figure(figsize = (20,20))
stopwords = set(STOPWORDS)
wc = WordCloud(background_color="white", stopwords=stopwords, width = 1600 , height = 800 , max_words = 3000).generate(" ".join(df4[df4.fraudulent == 1]['description']))
plt.axis("off")
plt.imshow(wc , interpolation = 'bilinear')
plt.savefig('fraud_cloud.jpeg')

df_clean = df4.copy()

df_clean.head()

"""# Making Graphs"""

plt.figure(figsize=(8,6))
plt.title('Distribution of Target Variable')
sns.countplot(x='fraudulent',data=df_clean)

df["function"].value_counts().plot(kind='barh', figsize=(8, 10), color='red', zorder=2, width=0.85)
plt.title('Most Represented Job Functions')

df["industry"].value_counts().head(n=20).plot(kind='barh', figsize=(8, 10), color='red', zorder=2, width=0.85)
plt.title('Most Represented Job Industries')

df["required_education"].value_counts().plot(kind='barh', figsize=(8, 10), color='red', zorder=2, width=0.85)
plt.title('Required Education')

df["employment_type"].value_counts().plot(kind='barh', figsize=(8, 10), color='orange', zorder=2, width=0.85)
plt.title('Employment Type')

df["required_experience"].value_counts().plot(kind='barh', figsize=(8, 10), color='cyan', zorder=2, width=0.85)
plt.title('Required Experience')

import spacy
import en_core_web_lg
from nltk.corpus import stopwords 
nlp = en_core_web_lg.load()
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))
punctuations = string.punctuation
def cleanup_text(docs, logging = False):
    texts = []
    for doc in docs:
        doc = nlp(doc, disable = ['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]
        tokens = ' '.join(tokens)
        texts.append(tokens)
    return pd.Series(texts)

Fraud_1 = [text for text in df_clean[df_clean['fraudulent'] == 1]['description']]
Fraud_1_clean = cleanup_text(Fraud_1)
Fraud_1_clean = ' '.join(Fraud_1_clean).split()
Fraud_1_counts = Counter(Fraud_1_clean)
Fraud_1_common_words = [word[0] for word in Fraud_1_counts.most_common(20)]
Fraud_1_common_counts = [word[1] for word in Fraud_1_counts.most_common(20)]
fig = plt.figure(figsize = (20, 10))
pal = sns.color_palette("cubehelix", 20)
sns.barplot(x = Fraud_1_common_words, y = Fraud_1_common_counts, palette=pal)
plt.title('Most Common Words used in Fake job postings')
plt.ylabel("Frequency of words")
plt.xlabel("Words")
plt.show()

Fraud_0 = [text for text in df_clean[df_clean['fraudulent'] == 0]['description']]
Fraud_0_clean = cleanup_text(Fraud_0)
Fraud_0_clean = ' '.join(Fraud_0_clean).split()
Fraud_0_counts = Counter(Fraud_0_clean)
Fraud_0_common_words = [word[0] for word in Fraud_0_counts.most_common(20)]
Fraud_0_common_counts = [word[1] for word in Fraud_0_counts.most_common(20)]
fig = plt.figure(figsize = (20, 10))
pal = sns.color_palette("cubehelix", 20)
sns.barplot(x = Fraud_0_common_words, y = Fraud_0_common_counts, palette=pal)
plt.title('Most Common Words used in Genuine job postings')
plt.ylabel("Frequency of words")
plt.xlabel("Words")
plt.show()

"""# Featurizing the Data"""

STOPLIST = stopwords
SYMBOLS = " ".join(string.punctuation).split(" ")
def tokenizetext(docs, logging = False):
    texts = []
    for doc in docs:
        doc = nlp(doc, disable = ['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]
        tokens = ' '.join(tokens)
        texts.append(tokens)
        return pd.Series(texts)
vectorizer = CountVectorizer(tokenizer = tokenizetext, ngram_range = (1,3), min_df = 0.06)
vectorizer_features = vectorizer.fit_transform(df_clean['description'])
vectorized_df = pd.DataFrame(vectorizer_features.todense(), columns = vectorizer.get_feature_names())
df_final = pd.concat([df_clean, vectorized_df], axis = 1)
df_final.drop('description', axis = 1, inplace = True)
df_final.dropna(inplace=True)
columns_to_1_hot = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']
for column in columns_to_1_hot:
    encoded = pd.get_dummies(df_final[column])
    df_final = pd.concat([df_final, encoded], axis = 1)
columns_to_1_hot += ['title', 'city', 'country_name']
df_final.drop(columns_to_1_hot, axis = 1, inplace = True)

"""# Decision Tree Classifier"""

X = df_final.drop('fraudulent', axis = 1).copy()
y = df_final['fraudulent'].copy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) 
clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

plot_confusion_matrix(clf, X_test, y_test, values_format = 'd', display_labels=["Real","Fraudulent"])
plt.title('Confusion Matrix')

"""# Bayes Regression"""

df.head()

del df['job_id']
del df['salary_range']

df.fillna(" ",inplace = True)

df['textdata'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function']

del df['title']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['employment_type']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']

df.head()

nltk.download('stopwords')

import spacy
import en_core_web_lg
from nltk.corpus import stopwords 
nlp = en_core_web_lg.load()
nltk.download('stopwords')
stop = set(stopwords.words('english'))
len(stop)

punctuation = list(string.punctuation)
stop.update(punctuation)

def cleaner(phrase):
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can't", 'can not', phrase)
    phrase = re.sub(r"n\'t"," not", phrase)
    phrase = re.sub(r"\'re'"," are", phrase)
    phrase = re.sub(r"\'s"," is", phrase)
    phrase = re.sub(r"\'ll"," will", phrase)
    phrase = re.sub(r"\'d"," would", phrase)
    phrase = re.sub(r"\'t"," not", phrase)
    phrase = re.sub(r"\'ve"," have", phrase)
    phrase = re.sub(r"\'m"," am", phrase)
    
    return phrase

cleaned_title = []

for sentance in tqdm(df['textdata'].values):
    sentance = str(sentance)
    sentance = re.sub(r"http\S+", "", sentance)
    sentance = BeautifulSoup(sentance, 'lxml').get_text()
    sentance = cleaner(sentance)
    sentance = re.sub(r'[?|!|\'|"|#|+]', r'', sentance)
    sentance = re.sub("\S*\d\S*", "", sentance).strip()
    sentance = re.sub('[^A-Za-z]+', ' ', sentance)
    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)
    cleaned_title.append(sentance.strip())

df['textdata'] = cleaned_title

df.head()

plt.figure(figsize = (20,20)) # Text that is not fraudulent(0)
wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(" ".join(df[df.fraudulent == 0].textdata))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is  fraudulent(1)
wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(" ".join(df5[df.fraudulent == 1].textdata))
plt.imshow(wc , interpolation = 'bilinear')

sns.countplot(x = "fraudulent", data=df)

X_Train, X_test, y_Train, y_test = train_test_split(df.textdata, df.fraudulent, random_state=0, stratify=df.fraudulent, test_size=0.1)

X_Train.shape

X_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, random_state=0, stratify=y_Train, test_size=0.1)

y_train.shape

tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))
#transformed train reviews
tv_train_reviews=tv.fit_transform(X_train)
#transformed test reviews

tv_cross_reviews=tv.transform(X_cross)
print('Tfidf_train:',tv_train_reviews.shape)
print('Tfidf_test:',tv_cross_reviews.shape)

tv_test_reviews=tv.transform(X_test)

alpha_set=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]
Train_AUC_BOW = []
CrossVal_AUC_BOW = []
for i in alpha_set:
    naive_b=MultinomialNB(alpha=i)
    naive_b.fit(tv_train_reviews, y_train)
    Train_y_pred =  naive_b.predict(tv_train_reviews)
    Train_AUC_BOW.append(roc_auc_score(y_train,Train_y_pred))
    CrossVal_y_pred =  naive_b.predict(tv_cross_reviews)
    CrossVal_AUC_BOW.append(roc_auc_score(y_cross,CrossVal_y_pred))

Alpha_set=[]
for i in range(len(alpha_set)):
    Alpha_set.append(math.log(alpha_set[i]))

plt.plot(Alpha_set, Train_AUC_BOW, label='Train AUC')
plt.scatter(Alpha_set, Train_AUC_BOW)
plt.plot(Alpha_set, CrossVal_AUC_BOW, label='CrossVal AUC')
plt.scatter(Alpha_set, CrossVal_AUC_BOW)
plt.legend()
plt.xlabel("alpha : hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.show()

optimal_alpha=alpha_set[CrossVal_AUC_BOW.index(max(CrossVal_AUC_BOW))]
print(optimal_alpha)

Classifier1=MultinomialNB(alpha=optimal_alpha)
Classifier1.fit(tv_train_reviews, y_train)

auc_train_bow = roc_auc_score(y_train,Classifier1.predict(tv_train_reviews))
print ("AUC for Train set", auc_train_bow)

auc_test_bow = roc_auc_score(y_test,Classifier1.predict(tv_test_reviews))
print ("AUC for Test set",auc_test_bow)

preds = Classifier1.predict(tv_test_reviews)
acc = accuracy_score(y_test, preds)
f1 = f1_score(y_test, preds, average='macro')
print ('Accuracy is : ', acc)
print ('F1 Score is :', f1)

print('Confusion Matrix of Test Data')
Test_mat=confusion_matrix(y_test,preds)
print (Test_mat)

print(metrics.classification_report(y_test, preds))

cm_cv = pd.DataFrame(Test_mat, index=[0,1], columns=[0,1])
cm_cv.index.name = 'Actual'
cm_cv.columns.name = 'Predicted'

plt.figure(figsize = (10,10))
sns.heatmap(cm_cv,cmap= "Blues",annot = True )

#Random Forest

X = df_final.drop('fraudulent', axis = 1).copy()
y = df_final['fraudulent'].copy()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) 
from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(n_estimators=100)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf, X_test, y_test, values_format = 'd', display_labels=["Real","Fraudulent"])
plt.title('Confusion Matrix')

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
print(classification_report(y_test, y_pred))

#XG Boost Regression

# In this first part I am cleaning the data for XGBoost since it doesn't accept certain types of data like objects

df_clean2 = df4.copy()

df_clean2.info()

df_clean2.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df_clean2.columns.values]
df_clean2.columns = df_clean.columns.map(str)
X = df_clean2.drop('fraudulent', axis = 1).copy()
X.head()

y = df_clean2['fraudulent'].copy()
y.head()

#Here I am getting the dummy variables which I will later use in my code

X_encoded = pd.get_dummies(X, columns = ['title','description', 'employment_type', 'required_experience', 'required_education','industry', 'function', 'city', 'country_name'])

y.unique()

sum(y)/len(y)

#Making the train/test split

X_train,X_test,y_train,y_test = train_test_split(X_encoded, y , random_state=42, stratify = y)

sum(y_train)/len(y_train)

sum(y_test)/len(y_test)

#Running the regression

regex = re.compile(r"\[|\]|<", re.IGNORECASE)
X_train.columns = X_train.columns.map(str)
X_test.columns = X_test.columns.map(str)
X_train.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]
X_test.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]
clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', missing = None, seed = 42)
clf_xgb.fit(X_train,y_train, verbose= True, early_stopping_rounds = 10, eval_metric = 'aucpr', eval_set = [(X_test, y_test)])

plot_confusion_matrix(clf_xgb, X_test, y_test, values_format = 'd', display_labels=["Real","Fraudulent"])
plt.title('Confusion Matrix')

y_pred = clf_xgb.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
print(classification_report(y_test, y_pred))

"""# Support Vector Machine"""

# Import libraries
from sklearn.pipeline import Pipeline
from sklearn.pipeline import Pipeline

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Data Pre-Processing
df=pd.read_csv('gdrive/My Drive/fake_job_postings.csv')
data = df
data.interpolate(inplace=True)
data.fillna(' ', inplace=True)
data['corpus'] = data['title'] + ' ' + data['location'] + ' ' + data['company_profile'] + ' ' + data['description'] + ' ' + data['requirements'] + ' ' + data['benefits']

del data['title']
del data['location']
del data['department']
del data['company_profile']
del data['description']
del data['requirements']
del data['benefits']
del data['required_experience']
del data['required_education']
del data['industry']
del data['function']

X : list = data.corpus.tolist()
y : list = data.fraudulent.tolist()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Training
pipeline : Pipeline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,3))),
                 ('classifier', SVC())])
pipeline.fit(X_train, y_train)

pipeline.fit(X_train, y_train)

# Model Evaluation
y_pred = pipeline.predict(X_test)
print('Accuracy: ', accuracy_score(y_test, y_pred), '\n\n')
print('-- Report --\n\n')
print(classification_report(y_test, y_pred))

# Plot confusion matrix
matrix = plot_confusion_matrix(pipeline, X_test, y_test)
plt.title('Confusion matrix')
plt.show(matrix)
plt.show()

"""# Logistic Regression"""

"""Importing libraries"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')

from tqdm import tqdm
import time
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold,KFold, cross_val_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
import xgboost as xgb

from sklearn import preprocessing, model_selection, pipeline
from sklearn.metrics import f1_score, roc_auc_score

"Missing Values"
from google.colab import drive
drive.mount('/content/gdrive')
import pandas as pd
df=pd.read_csv('gdrive/My Drive/fake_job_postings.csv')
df.isnull().sum()

"Extracting Text Features"

text_df = df[["title", "company_profile", "description", "requirements", "benefits","fraudulent"]]
text_df = text_df.fillna(' ')

text_df.head()

"Categorical Feature"
cat_df = df[["telecommuting", "has_company_logo", "has_questions", "employment_type", "required_experience", "required_education", "industry", "function","fraudulent"]]
cat_df = cat_df.fillna("None")

cat_df.head()

"""# Target"""

fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
plt.tight_layout()

df["fraudulent"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post (95%)', 'Fake Post (5%)'])
temp = df["fraudulent"].value_counts()
sns.barplot(temp.index, temp, ax=axes[1])

axes[0].set_ylabel(' ')
axes[1].set_ylabel(' ')
axes[1].set_xticklabels(["Real Post (17014) [0's]", "Fake Post (866) [1's]"])

axes[0].set_title('Target Distribution in Dataset', fontsize=13)
axes[1].set_title('Target Count in Dataset', fontsize=13)

plt.show()

"""Hmm!! Class distributions are **95% for 0 (Real Post)** and **5% for 1 (Fake Post).** Target distribution is highly imbalanced. Accuracy metric is not useful here it will mislead the result. So, we've to look into Precision, Recall, F1 Score for model evalution.

# Exploratory Data Analysis of tweets
"""

cat_cols = ["telecommuting", "has_company_logo", "has_questions", "employment_type", "required_experience", "required_education",]
# visualizating categorical variable by target
 # to do the grid of plots
grid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5) # The grid of chart
plt.figure(figsize=(15,25)) # size of figure

# loop to get column and the count of plots
for n, col in enumerate(cat_df[cat_cols]): 
    ax = plt.subplot(grid[n]) # feeding the figure of grid
    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='Set2') 
    ax.set_ylabel('Count', fontsize=12) # y axis label
    ax.set_title(f'{col} Distribution by Target', fontsize=15) # title label
    ax.set_xlabel(f'{col} values', fontsize=12) # x axis label
    xlabels = ax.get_xticklabels() 
    ylabels = ax.get_yticklabels() 
    ax.set_xticklabels(xlabels,  fontsize=10)
    ax.set_yticklabels(ylabels,  fontsize=10)
    plt.legend(fontsize=8)
    plt.xticks(rotation=90) 
    total = len(cat_df)
    sizes=[] # Get highest values in y
    for p in ax.patches: # loop to all objects
        height = p.get_height()
        sizes.append(height)
        ax.text(p.get_x()+p.get_width()/2.,
                height + 3,
                '{:1.2f}%'.format(height/total*100),
                ha="center", fontsize=10) 
    ax.set_ylim(0, max(sizes) * 1.15) #set y limit based on highest heights


plt.show()

"""## Number of characters
Let's compare the number of character in the fake post and real post and try to distinguish pattern in the fake and real post based on number of charater used in the post.

#### Company profile
We can see that **fake post has less characters** in the company profile while **real post has more charaters.**

#### Description
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
length=text_df[text_df["fraudulent"]==1]['description'].str.len()
ax1.hist(length,bins = 20,color='orangered')
ax1.set_title('Fake Post')
length=text_df[text_df["fraudulent"]==0]['description'].str.len()
ax2.hist(length, bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Characters in description')
plt.show()

"""The distribution of charaters in description of the fake and real post are similar but some fake post reach to 6000 to 6500 characters.

#### Requirements
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
length=text_df[text_df["fraudulent"]==1]['requirements'].str.len()
ax1.hist(length,bins = 20,color='orangered')
ax1.set_title('Fake Post')
length=text_df[text_df["fraudulent"]==0]['requirements'].str.len()
ax2.hist(length,bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Characters in requirements')
plt.show()

"""The distribution of charaters in requirements of the fake and real post are similar.

#### Benefits
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
length=text_df[text_df["fraudulent"]==1]['benefits'].str.len()
ax1.hist(length,bins = 20,color='orangered')
ax1.set_title('Fake Post')
length=text_df[text_df["fraudulent"]==0]['benefits'].str.len()
ax2.hist(length,bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Characters in benefits')
plt.show()

"""The distribution of charaters in requirements of the fake and real post is same around 1500 to 1800.

## Number of words
Let's compare the number of words in the fake post and real post and try to distinguish pattern in the fake and real post based on number of words used in the post.

#### Company Profile
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
num=text_df[text_df["fraudulent"]==1]['company_profile'].str.split().map(lambda x: len(x))
ax1.hist(num,bins = 20,color='orangered')
ax1.set_title('Fake Post')
num=text_df[text_df["fraudulent"]==0]['company_profile'].str.split().map(lambda x: len(x))
ax2.hist(num, bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Words in company profile')
plt.show()

"""Pattern of words in company profile is same as character in company profile. **fake post has less words** in the company profile while **real post has more words.**

#### Description
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
num=text_df[text_df["fraudulent"]==1]['description'].str.split().map(lambda x: len(x))
ax1.hist(num,bins = 20,color='orangered')
ax1.set_title('Fake Post')
num=text_df[text_df["fraudulent"]==0]['description'].str.split().map(lambda x: len(x))
ax2.hist(num, bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Words in description')
plt.show()

"""Hmm!! Both the post has similar distribution of words in description.

#### Requirements
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
num=text_df[text_df["fraudulent"]==1]['requirements'].str.split().map(lambda x: len(x))
ax1.hist(num,bins = 20,color='orangered')
ax1.set_title('Fake Post')
num=text_df[text_df["fraudulent"]==0]['requirements'].str.split().map(lambda x: len(x))
ax2.hist(num,bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Words in requirements')
plt.show()

"""The distribution of words in requirements of the fake and real post are similar.

#### Benefits
"""

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
num=text_df[text_df["fraudulent"]==1]['benefits'].str.split().map(lambda x: len(x))
ax1.hist(num,bins = 20,color='orangered')
ax1.set_title('Fake Post')
num=text_df[text_df["fraudulent"]==0]['benefits'].str.split().map(lambda x: len(x))
ax2.hist(num, bins = 20)
ax2.set_title('Real Post')
fig.suptitle('Words in benefits')
plt.show()

"""The distribution of words in benefits of the fake and real post are also similar.

# Data preprocessing
"""

"Concate the text data for preprocessing and modeling"
text = text_df[text_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)
target = df['fraudulent']

print(len(text))
print(len(target))

"""## Ngrams Analysis

Most common unigrams exist in **both classes** are mostly punctuations, stop words or numbers. It is better to clean them before modelling since they don't give much information about target.
"""

def get_top_tweet_unigrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

def get_top_tweet_bigrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

fig, axes = plt.subplots(ncols=2, figsize=(18, 30), dpi=100)
plt.tight_layout()

top_unigrams=get_top_tweet_unigrams(text)[:50]
x,y=map(list,zip(*top_unigrams))
sns.barplot(x=y,y=x, ax=axes[0], color='teal')


top_bigrams=get_top_tweet_bigrams(text)[:50]
x,y=map(list,zip(*top_bigrams))
sns.barplot(x=y,y=x, ax=axes[1], color='crimson')


axes[0].set_ylabel(' ')
axes[1].set_ylabel(' ')

axes[0].set_title('Top 50 most common unigrams in text', fontsize=15)
axes[1].set_title('Top 50 most common bigrams in text', fontsize=15)

plt.show()

"""## Text cleaning"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def clean_text(text):
#     '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
#     and remove words containing numbers.'''
#     text = text.lower()
#     text = re.sub('\[.*?\]', '', text)
#     text = re.sub('https?://\S+|www\.\S+', '', text)
#     text = re.sub('<.*?>+', '', text)
#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
#     text = re.sub('\n', '', text)
#     text = re.sub('\w*\d\w*', '', text)
#     return text
# 
# 
# # Applying the cleaning function to both test and training datasets
# text = text.apply(lambda x: clean_text(x))
# text.head(3)

"""## Tokenizer"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
# 
# # appling tokenizer
# text = text.apply(lambda x: tokenizer.tokenize(x))
# text.head(3)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# stop_words = stopwords.words('english')
# def remove_stopwords(text):
#     """
#     Removing stopwords belonging to english language
#     
#     """
#     words = [w for w in text if w not in stop_words]
#     return words
# 
# 
# text = text.apply(lambda x : remove_stopwords(x))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def combine_text(list_of_text):
#     combined_text = ' '.join(list_of_text)
#     return combined_text
# 
# text = text.apply(lambda x : combine_text(x))
# text.head(3)

"""# Baseline Model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)
# auc_buf = []   
# cnt = 0
# predictions = 0
# # enumerate the splits and summarize the distributions
# for train_ix, test_ix in kfold.split(text, target):
#     print('Fold {}'.format(cnt + 1))
#     train_X, test_X = text[train_ix], text[test_ix]
#     train_y, test_y = target[train_ix], target[test_ix]
# 
#     # Appling Count Vectorizer
#     count_vectorizer = CountVectorizer()
#     train_X_vec = count_vectorizer.fit_transform(train_X)
#     test_X_vec = count_vectorizer.transform(test_X)    
#     
#     lr = LogisticRegression(C=0.1, solver='lbfgs', max_iter=1000, verbose=0, n_jobs=-1)
# #     https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm
#     lr.fit(train_X_vec, train_y)
#     preds = lr.predict(test_X_vec)
#     
#     auc = roc_auc_score(test_y, preds)
#     print('{} AUC: {}'.format(cnt, auc))
#     auc_buf.append(auc)
#     cnt += 1
# 
# print('AUC mean score = {:.6f}'.format(np.mean(auc_buf)))
# print('AUC std score = {:.6f}'.format(np.std(auc_buf)))

"""## Confusion Matrix"""

from sklearn.metrics import classification_report,plot_confusion_matrix,accuracy_score
plot_confusion_matrix(lr, test_X_vec, test_y, values_format = 'd', display_labels=["Real","Frauduluent"])
plt.grid(False)
plt.title('Confusion Matrix')

plt.show()

print(classification_report(test_y, preds))

